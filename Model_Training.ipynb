{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(img):\n",
    "  img = cv2.resize(img, dsize=(84, 84))\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnvironment(gym.Wrapper):\n",
    "  def __init__(self, env, skip_frames=2, stack_frames=4, no_operation=5, **kwargs):\n",
    "    super().__init__(env, **kwargs)\n",
    "    self._no_operation = no_operation\n",
    "    self._skip_frames = skip_frames\n",
    "    self._stack_frames = stack_frames\n",
    "\n",
    "  def reset(self):\n",
    "    observation, info = self.env.reset()\n",
    "\n",
    "    for i in range(self._no_operation):\n",
    "      observation, reward, terminated, truncated, info = self.env.step(0)\n",
    "\n",
    "    observation = image_preprocessing(observation)\n",
    "    self.stack_state = np.tile(observation, (self._stack_frames, 1, 1))\n",
    "    return self.stack_state, info\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    total_reward = 0\n",
    "    for i in range(self._skip_frames):\n",
    "      observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "      total_reward += reward\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "    observation = image_preprocessing(observation)\n",
    "    self.stack_state = np.concatenate((self.stack_state[1:], observation[np.newaxis]), axis=0)\n",
    "    return self.stack_state, total_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self._n_features = 32 * 9 * 9\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(self._n_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, out_channels),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = x.view((-1, self._n_features))\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  def __init__(self, action_space, batch_size=256, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=1000, lr=0.001, use_vit=False):\n",
    "      self._n_observation = 4\n",
    "      self._n_actions = 5\n",
    "      self._action_space = action_space\n",
    "      self._batch_size = batch_size\n",
    "      self._gamma = gamma\n",
    "      self._eps_start = eps_start\n",
    "      self._eps_end = eps_end\n",
    "      self._eps_decay = eps_decay\n",
    "      self._lr = lr\n",
    "      self._total_steps = 0\n",
    "      self._evaluate_loss = []\n",
    "      self.use_vit = use_vit  \n",
    "      \n",
    "      # 네트워크 초기화\n",
    "      self.network = self.build_network()\n",
    "      self.target_network = self.build_network()\n",
    "      self.target_network.load_state_dict(self.network.state_dict())\n",
    "      self.optimizer = optim.AdamW(self.network.parameters(), lr=self._lr, amsgrad=True)\n",
    "      self._memory = ReplayMemory(10000)  \n",
    "      \n",
    "      # 초기 네트워크 파라미터 저장 (Shrink & Perturb용)\n",
    "      self.initial_params = self.network.state_dict()  \n",
    "      \n",
    "  def build_network(self):\n",
    "        if self.use_vit:\n",
    "            # ViT 모델을 weights 매개변수를 사용하여 불러옴\n",
    "            weights = ViT_B_16_Weights.DEFAULT\n",
    "            model = vit_b_16(weights=weights)\n",
    "            model.heads.head = nn.Linear(model.heads.head.in_features, self._n_actions)\n",
    "            return model.to(device)\n",
    "        else:\n",
    "            return CNN(self._n_observation, self._n_actions).to(device)\n",
    "\n",
    "  \"\"\"\n",
    "  This function is called during training & evaluation phase when the agent\n",
    "  interact with the environment and needs to select an action.\n",
    "\n",
    "  (1) Exploitation: This function feeds the neural network a state\n",
    "  and then it selects the action with the highest Q-value.\n",
    "  (2) Evaluation mode: This function feeds the neural network a state\n",
    "  and then it selects the action with the highest Q'-value.\n",
    "  (3) Exploration mode: It randomly selects an action through sampling\n",
    "\n",
    "  Q -> network (policy)\n",
    "  Q'-> target network (best policy)\n",
    "  \"\"\"\n",
    "  def select_action(self, state, evaluation_phase=False):\n",
    "    # Generating a random number for eploration vs exploitation\n",
    "    sample = random.random()\n",
    "\n",
    "    # Calculating the threshold - the more steps the less exploration we do\n",
    "    eps_threshold = self._eps_end + (self._eps_start - self._eps_end) * math.exp(-1. * self._total_steps / self._eps_decay)\n",
    "    self._total_steps += 1\n",
    "\n",
    "    if evaluation_phase:\n",
    "      with torch.no_grad():\n",
    "        return self.target_network(state).max(1).indices.view(1, 1)\n",
    "    elif sample > eps_threshold:\n",
    "      with torch.no_grad():\n",
    "        return self.network(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "      return torch.tensor([[self._action_space.sample()]], device=device, dtype=torch.long)\n",
    "        \n",
    "\n",
    "  def train(self, replay_ratio=1):\n",
    "    if len(self._memory) < self._batch_size:\n",
    "        return\n",
    "        \n",
    "    for _ in range(replay_ratio):\n",
    "        # Initializing our memory\n",
    "        transitions = self._memory.sample(self._batch_size)\n",
    "        # Initializing our batch\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Saving in a new tensor all the indices of the states that are non terminal\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Feeding our Q network the batch with states and then we gather the Q values of the selected actions\n",
    "        state_action_values = self.network(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # We then, for every state in the batch that is NOT final, we pass it in the target network to get the Q'-values and choose the max one\n",
    "        next_state_values = torch.zeros(self._batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1).values\n",
    "\n",
    "        # Computing the expecting values with: reward + gamma * max(Q')\n",
    "        expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.network.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        self._evaluate_loss.append(loss.item())\n",
    "            \n",
    "  def shrink_and_perturb(self, alpha=0.9):\n",
    "    current_params = self.network.state_dict()\n",
    "    perturbed_params = {k: alpha * current_params[k] + (1 - alpha) * self.initial_params[k] for k in current_params}\n",
    "    self.network.load_state_dict(perturbed_params)\n",
    "      \n",
    "  def copy_weights(self):\n",
    "    self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "  def get_loss(self):\n",
    "    return self._evaluate_loss\n",
    "\n",
    "  def load_model(self, version, i):\n",
    "    self.target_network.load_state_dict(torch.load(f'model/{version}/model_weights_{i}.pth', map_location=device))\n",
    "\n",
    "  def load_model(self, version, i):\n",
    "    self.target_network.load_state_dict(torch.load(f'model/{version}/model_weights_{i}.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_envs_with_random_seeds(envs):\n",
    "    \"\"\"\n",
    "    각 환경에 랜덤 시드를 설정하고 초기화.\n",
    "    \"\"\"\n",
    "    new_seeds = [np.random.randint(1000) for _ in range(len(envs))]\n",
    "    states = []\n",
    "    for env, seed in zip(envs, new_seeds):\n",
    "        env.np_random = np.random.default_rng(seed)  # 랜덤 시드 재설정\n",
    "        state, _ = env.reset()  # 환경 초기화\n",
    "        states.append(torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "import yaml\n",
    "# YAML 파일 읽기\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 설정값 가져오기\n",
    "training_params = config['training']\n",
    "env_params = config['environment']\n",
    "\n",
    "# 초기 시드 설정\n",
    "np.random.seed(42)  # NumPy 랜덤 시드 고정\n",
    "torch.manual_seed(42)  # PyTorch 랜덤 시드 고정\n",
    "\n",
    "# 환경 및 파라미터 설정\n",
    "env_count = env_params['env_count']\n",
    "TU = training_params['TU']\n",
    "TC = training_params['TC']\n",
    "TR = training_params['TR']\n",
    "map_reset_interval = training_params['map_reset_interval']\n",
    "replay_ratio = training_params['replay_ratio']\n",
    "episodes = training_params['episodes']\n",
    "alpha = training_params['alpha']\n",
    "save_point = training_params['save_point']\n",
    "\n",
    "\n",
    "version=f\"basemodel_{map_reset_interval}_{replay_ratio}_{alpha}_{TU}_{TR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dlj1ojrx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Duration</td><td>▁▁</td></tr><tr><td>Average Loss</td><td>█▁</td></tr><tr><td>Average Reward</td><td>▁█</td></tr><tr><td>Episode</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Duration</td><td>497.0</td></tr><tr><td>Average Loss</td><td>1.77912</td></tr><tr><td>Average Reward</td><td>-43.68342</td></tr><tr><td>Episode</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">basemodel_50_10_0.9_3_50</strong> at: <a href='https://wandb.ai/tatalintelli-university-of-seoul/car-racing/runs/dlj1ojrx' target=\"_blank\">https://wandb.ai/tatalintelli-university-of-seoul/car-racing/runs/dlj1ojrx</a><br/> View project at: <a href='https://wandb.ai/tatalintelli-university-of-seoul/car-racing' target=\"_blank\">https://wandb.ai/tatalintelli-university-of-seoul/car-racing</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241118_161259-dlj1ojrx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dlj1ojrx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7c3eefef3448bfb1ea93e5963feced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112335924473074, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs/home1/alsrn50/CIDA_LABS/car_racing_project/wandb/run-20241118_161510-dturarca</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tatalintelli-university-of-seoul/F1024%20Project%20%28Car-racing%29/runs/dturarca' target=\"_blank\">basemodel_50_10_0.9_3_50</a></strong> to <a href='https://wandb.ai/tatalintelli-university-of-seoul/F1024%20Project%20%28Car-racing%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tatalintelli-university-of-seoul/F1024%20Project%20%28Car-racing%29' target=\"_blank\">https://wandb.ai/tatalintelli-university-of-seoul/F1024%20Project%20%28Car-racing%29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tatalintelli-university-of-seoul/F1024%20Project%20%28Car-racing%29/runs/dturarca' target=\"_blank\">https://wandb.ai/tatalintelli-university-of-seoul/F1024%20Project%20%28Car-racing%29/runs/dturarca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tatalintelli-university-of-seoul/F1024%20Project%20%28Car-racing%29/runs/dturarca?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fad850db4d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb로 트래킹\n",
    "import wandb\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    project=\"F1024 Project (Car-racing)\",  # 프로젝트 이름\n",
    "    name=version,          # 실험 이름\n",
    "    config={               # 설정값 로깅\n",
    "        \"TU\": TU,\n",
    "        \"TC\": TC,\n",
    "        \"TR\": TR,\n",
    "        \"map_reset_interval\": map_reset_interval,\n",
    "        \"replay_ratio\": replay_ratio,\n",
    "        \"episodes\": episodes,\n",
    "        \"alpha\": alpha,\n",
    "        \"env_count\": env_count\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|███▊                                                                                                                                                                                                                                    | 13/800 [08:00<8:04:34, 36.94s/it]"
     ]
    }
   ],
   "source": [
    "# 병렬 환경 생성\n",
    "\n",
    "envs = [CarEnvironment(gym.make('CarRacing-v2', continuous=False)) for _ in range(env_count)]  # 최초 환경 생성\n",
    "agent = DQN(envs[0].action_space, use_vit=False)\n",
    "\n",
    "rewards_per_episode = []\n",
    "episode_duration = []\n",
    "average_episode_loss = []\n",
    "\n",
    "for episode in tqdm(range(1, episodes + 1), desc=\"Training\"):\n",
    "\n",
    "    # 특정 주기마다 맵 변경\n",
    "    if episode % map_reset_interval == 1:  # 맵 변경 주기 시작\n",
    "        states = reset_envs_with_random_seeds(envs)\n",
    "    else:\n",
    "        states = [env.reset()[0] for env in envs]  # 기존 방식으로 환경 초기화\n",
    "        states = [torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0) for s in states]\n",
    "\n",
    "    episode_total_rewards = [0] * len(envs)  # 각 환경의 총 보상 저장\n",
    "    episode_losses = []  # 그냥 Replay_ratio 수만큼 loss 리스트로 저장\n",
    "    episode_durations = [0] * len(envs)  # 각 환경의 지속 시간 저장\n",
    "    done_mask = [False] * len(envs)  # 환경별 종료 상태 트래킹\n",
    "\n",
    "    for t in range(500):  # 무한 루프\n",
    "        # CNN에 전달하기 위해 states를 텐서(batch)로 변환\n",
    "        # 종료되지 않은 환경 상태를 결합\n",
    "        valid_indices = [i for i in range(len(envs)) if not done_mask[i]]\n",
    "        valid_states = [states[i] for i in valid_indices if states[i] is not None]\n",
    "        if len(valid_states) == 0:  # 모든 환경이 종료된 경우\n",
    "            break\n",
    "        states_tensor = torch.cat(valid_states, dim=0)\n",
    "        \n",
    "        # 에이전트가 행동 선택\n",
    "        actions = [\n",
    "            agent.select_action(states_tensor[idx].unsqueeze(0))\n",
    "            for idx in range(len(valid_indices))\n",
    "        ]\n",
    "        # 환경 상호작용 (종료되지 않은 환경만)\n",
    "        for idx, env_idx in enumerate(valid_indices):\n",
    "            next_state, reward, terminated, truncated, _ = envs[env_idx].step(actions[idx].item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done_flags = terminated or truncated\n",
    "    \n",
    "            if not done_flags:  # 환경이 종료되지 않은 경우 업데이트\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                agent._memory.push(states[env_idx], actions[idx], next_state, reward)\n",
    "                states[env_idx] = next_state  # 상태 업데이트\n",
    "                episode_total_rewards[env_idx] += reward\n",
    "                episode_durations[env_idx] += 1\n",
    "            else:\n",
    "                states[env_idx] = None  # 종료된 환경은 상태를 None으로 유지\n",
    "                done_mask[env_idx] = True  # 종료 상태 갱신\n",
    "\n",
    "        # 학습 수행 후 손실을 기록\n",
    "        if t % TU == 0:\n",
    "            agent.train(replay_ratio)\n",
    "            # 종료되지 않은 환경에 대해 손실 기록\n",
    "            loss_values = agent.get_loss()\n",
    "            if loss_values:  # 손실 리스트가 비어 있지 않은 경우\n",
    "                episode_losses.append(loss_values)\n",
    "                \n",
    "        # 모든 환경이 종료되었는지 확인\n",
    "        if all(done_mask):\n",
    "            # 각 환경의 총 보상과 손실 평균을 계산하여 저장\n",
    "            avg_reward = sum(episode_total_rewards) / len(envs)\n",
    "            avg_loss = sum([sum(losses) / len(losses) for losses in episode_losses if losses]) / len(envs)\n",
    "            avg_duration = sum(episode_durations) / len(envs)  # 평균 지속 시간 계산\n",
    "            \n",
    "            # 평균 보상 및 손실을 저장\n",
    "            rewards_per_episode.append(avg_reward)\n",
    "            average_episode_loss.append(avg_loss)\n",
    "            episode_duration.append(avg_duration)\n",
    "\n",
    "            # 보상, 손실, 지속 시간을 각각 로깅\n",
    "            wandb.log({\n",
    "                \"Episode\": episode,           # x축 값\n",
    "                \"Average Reward\": avg_reward, # 그래프 1: 평균 보상\n",
    "                \"Average Loss\": avg_loss,     # 그래프 2: 평균 손실\n",
    "                \"Average Duration\": avg_duration  # 그래프 3: 평균 지속 시간\n",
    "            })\n",
    "            break\n",
    "            \n",
    "    if episode % TC == 0:\n",
    "        agent.copy_weights()\n",
    "\n",
    "    if episode % TR == 0:\n",
    "        agent.shrink_and_perturb(alpha)\n",
    "\n",
    "    if episode % save_point == 0:\n",
    "        print(f\"{episode} episodes done\")\n",
    "        agent.save_model(version, episode)\n",
    "        with open('statistics.pkl', 'wb') as f:\n",
    "            pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)\n",
    "\n",
    "agent.save_model(version, episode)\n",
    "with open('statistics.pkl', 'wb') as f:\n",
    "    pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_statistics(x, y, title, x_axis, y_axis):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{title.replace(\" \", \"_\")}.png')  # 공백 대신 밑줄 사용\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "eval_env = CarEnvironment(eval_env)\n",
    "n_actions = eval_env.action_space\n",
    "agent = DQN(n_actions)\n",
    "agent.load_model(400)\n",
    "\n",
    "frames = []\n",
    "scores = 0\n",
    "s, _ = eval_env.reset()\n",
    "\n",
    "eval_env.np_random = np.random.default_rng(42)\n",
    "\n",
    "done, ret = False, 0\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "def render2img(_img): return PILImage.fromarray(_img, \"RGB\")\n",
    "handle = display(None, display_id=True)\n",
    "while not done:\n",
    "    _render = eval_env.render()\n",
    "    handle.update(render2img(_render))\n",
    "    frames.append(_render)\n",
    "    s = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    a = agent.select_action(s, evaluation_phase=True)\n",
    "    discrete_action = a.item() % 5\n",
    "    s_prime, r, terminated, truncated, info = eval_env.step(discrete_action)\n",
    "    s = s_prime\n",
    "    ret += r\n",
    "    done = terminated or truncated\n",
    "    if terminated:\n",
    "      print(terminated)\n",
    "      \n",
    "scores += ret\n",
    "\n",
    "print(scores)\n",
    "def animate(imgs, video_name, _return=True):\n",
    "    import cv2\n",
    "    import os\n",
    "    import string\n",
    "    import random\n",
    "\n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
    "\n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video.write(img)\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animate(frames, None)\n",
    "\n",
    "with open('statistics.pkl', 'rb') as f:\n",
    "    data_tuple = pickle.load(f)\n",
    "\n",
    "episode_duration, rewards_per_episode, average_episode_loss = data_tuple\n",
    "\n",
    "x = [k for k in range(episodes)]\n",
    "\n",
    "rewards_per_episode = [tensor.cpu() if tensor.is_cuda else tensor for tensor in rewards_per_episode]\n",
    "\n",
    "plot_statistics(x, episode_duration, \"Duration for episode\", \"Episode\", \"Duration\")\n",
    "plot_statistics(x, rewards_per_episode, \"Rewards for every episode\", \"Episode\", \"Reward\")\n",
    "plot_statistics(x, average_episode_loss, \"Average loss for every episode\", \"Episode\", \"Average Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
